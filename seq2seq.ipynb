{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq Model using keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "m3-hFN4k_liQ",
    "outputId": "d0f3a7d1-c222-4a93-ac0c-bc2bcc8bd3ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zQiAobKe_87v",
    "outputId": "a32ec3ff-dc99-4e9d-c15d-cad76c77238b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/content'"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Seq2seq model is a machine learning approach used mainly for language translation. This model has basically two components Encoder and Decoder. Both the Encoder and Decoder have their own LSTM layer where the input to the encoder is English sentences that collect all the information of the sentence. Then we pass this information to the LSTM layer of the decoder. However, decoder expects French sentences as the input. Finally, our target will be the French sentence one timestep ahead of the decoder inputs. So that at each timestep, the model will learn the corresponding word given the English sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data - 1. English vacabulary 2. French vacabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WloWwBg3AgQI"
   },
   "outputs": [],
   "source": [
    "data_path = \"/content/gdrive/My Drive/Machine-Translation-Seq2Seq-Keras-master/Machine-Translation-Seq2Seq-Keras-master/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "K66doe5RA162",
    "outputId": "14f876a6-7b26-4a2e-ccea-93e475cf891f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded\n"
     ]
    }
   ],
   "source": [
    "with open(data_path + \"/\" + \"small_vocab_en\", 'r') as f:\n",
    "    eng_sentences = f.read().split('\\n')\n",
    "    f.close()\n",
    "with open(data_path + \"/\" + \"small_vocab_fr\", 'r') as f:\n",
    "    fre_sentences = f.read().split('\\n')\n",
    "    f.close()\n",
    "print('Dataset Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "ms9VnSkmA7e7",
    "outputId": "39a1e356-85b6-4249-b018-5f42c603216d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english sentence 1: new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "fre_sentence 1: new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "\n",
      "\n",
      "english sentence 2: the united states is usually chilly during july , and it is usually freezing in november .\n",
      "fre_sentence 2: les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    print(f'english sentence {i+1}: {eng_sentences[i]}')\n",
    "    print(f'fre_sentence {i+1}: {fre_sentences[i]}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "lfo4HbRzBdDC",
    "outputId": "90fe43cd-4eaf-4924-8266-a0fb54f46a86"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Input, Embedding, LSTM, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "#from keras.losses import frerse_categorical_crossentropy\n",
    "\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "\n",
    "from keras.layers import LSTM\n",
    "\n",
    "from keras.layers import TimeDistributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MeMvGYaucOXy"
   },
   "source": [
    "#### Append 'START_ ' and ' _END' at the beginning and the end of the french sentences. So that our model can distinguish the start and end of the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h98EVpJwLipb"
   },
   "outputs": [],
   "source": [
    "fre_sentences = ['START_ ' + text + ' _END' for text in fre_sentences]\n",
    "# Create vocabulary of words\n",
    "all_eng_words=set()\n",
    "for eng in eng_sentences:\n",
    "    for word in eng.split():\n",
    "        if word not in all_eng_words:\n",
    "            all_eng_words.add(word)\n",
    "    \n",
    "all_french_words=set()\n",
    "for fr in fre_sentences:\n",
    "    for word in fr.split():\n",
    "        if word not in all_french_words:\n",
    "            all_french_words.add(word)\n",
    "input_words = sorted(list(all_eng_words))\n",
    "target_words = sorted(list(all_french_words))\n",
    "num_encoder_tokens = len(all_eng_words)\n",
    "num_decoder_tokens = len(all_french_words)\n",
    "# del all_eng_words, all_french_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary of word-index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S2VBY2WvNr1f"
   },
   "outputs": [],
   "source": [
    "input_token_index = {w:i for i, w in enumerate(input_words)}\n",
    "target_token_index = {w:i for i, w in enumerate(target_words)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The target sequence should be one timestep ahead of decoder inputs. Because the model takes starting word of the sentence as  input and predicts the next word in the sentence.\n",
    "- Each sequence of the target data is converted to one-hot code representation vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PfW8lsmOOwYo"
   },
   "outputs": [],
   "source": [
    "eng_seq_len = 30\n",
    "fre_seq_len = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DrybiocJKcKi"
   },
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    (len(eng_sentences), eng_seq_len),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(fre_sentences), fre_seq_len),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(fre_sentences), fre_seq_len, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "# generate data\n",
    "for i, (input_text, target_text) in enumerate(zip(eng_sentences, fre_sentences)):\n",
    "    for t, word in enumerate(input_text.split()):\n",
    "        encoder_input_data[i, t] = input_token_index[word]\n",
    "    for t, word in enumerate(target_text.split()):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t] = target_token_index[word]\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[word]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Dwxyx-Skmvgt",
    "outputId": "154cbc12-f6aa-4c5a-c650-571030d4bd8a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 357)"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_target_data[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the se2seq model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Encoder architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vP-K6dEjPGW3"
   },
   "outputs": [],
   "source": [
    "embedding_size = 50\n",
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "# English words embedding\n",
    "en_x=  Embedding(num_encoder_tokens, embedding_size)(encoder_inputs)\n",
    "# Encoder lstm\n",
    "encoder = LSTM(50, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(en_x)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Decoder architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vP-K6dEjPGW3"
   },
   "outputs": [],
   "source": [
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "# french word embeddings\n",
    "dex=  Embedding(num_decoder_tokens, embedding_size)\n",
    "final_dex= dex(decoder_inputs)\n",
    "# decoder lstm\n",
    "decoder_lstm = LSTM(50, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(final_dex,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "# While training, model takes eng and french words and outputs #translated french word\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "# rmsprop is preferred for nlp tasks\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420
    },
    "colab_type": "code",
    "id": "lO8av0m7By7l",
    "outputId": "ca05f366-3c1f-4347-887b-d49de15010f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 50)     11350       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 50)     17850       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 50), (None,  20200       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 50), ( 20200       embedding_2[0][0]                \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 357)    18207       lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 87,807\n",
      "Trainable params: 87,807\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Ultif7N-GflI",
    "outputId": "f37f188b-0218-4819-f12e-e50cb5db9224"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/50\n",
      "110288/110288 [==============================] - 152s 1ms/step - loss: 1.0718 - acc: 0.2460 - val_loss: 0.5431 - val_acc: 0.3314\n",
      "Epoch 2/50\n",
      "110288/110288 [==============================] - 148s 1ms/step - loss: 0.4689 - acc: 0.3510 - val_loss: 0.4214 - val_acc: 0.3635\n",
      "Epoch 3/50\n",
      "110288/110288 [==============================] - 147s 1ms/step - loss: 0.4025 - acc: 0.3689 - val_loss: 0.3827 - val_acc: 0.3740\n",
      "Epoch 4/50\n",
      "110288/110288 [==============================] - 146s 1ms/step - loss: 0.3692 - acc: 0.3790 - val_loss: 0.3562 - val_acc: 0.3810\n",
      "Epoch 5/50\n",
      "110288/110288 [==============================] - 149s 1ms/step - loss: 0.3404 - acc: 0.3886 - val_loss: 0.3251 - val_acc: 0.3932\n",
      "Epoch 6/50\n",
      "110288/110288 [==============================] - 144s 1ms/step - loss: 0.3138 - acc: 0.3975 - val_loss: 0.2983 - val_acc: 0.4020\n",
      "Epoch 7/50\n",
      "110288/110288 [==============================] - 144s 1ms/step - loss: 0.2879 - acc: 0.4061 - val_loss: 0.2713 - val_acc: 0.4110\n",
      "Epoch 8/50\n",
      "110288/110288 [==============================] - 145s 1ms/step - loss: 0.2609 - acc: 0.4161 - val_loss: 0.2506 - val_acc: 0.4196\n",
      "Epoch 9/50\n",
      "110288/110288 [==============================] - 146s 1ms/step - loss: 0.2324 - acc: 0.4270 - val_loss: 0.2195 - val_acc: 0.4318\n",
      "Epoch 10/50\n",
      "110288/110288 [==============================] - 147s 1ms/step - loss: 0.2045 - acc: 0.4367 - val_loss: 0.1890 - val_acc: 0.4416\n",
      "Epoch 11/50\n",
      "110288/110288 [==============================] - 143s 1ms/step - loss: 0.1812 - acc: 0.4452 - val_loss: 0.1692 - val_acc: 0.4493\n",
      "Epoch 12/50\n",
      "110288/110288 [==============================] - 147s 1ms/step - loss: 0.1611 - acc: 0.4527 - val_loss: 0.1613 - val_acc: 0.4496\n",
      "Epoch 13/50\n",
      "110288/110288 [==============================] - 145s 1ms/step - loss: 0.1436 - acc: 0.4596 - val_loss: 0.1327 - val_acc: 0.4636\n",
      "Epoch 14/50\n",
      "110288/110288 [==============================] - 146s 1ms/step - loss: 0.1286 - acc: 0.4656 - val_loss: 0.1287 - val_acc: 0.4638\n",
      "Epoch 15/50\n",
      "110288/110288 [==============================] - 146s 1ms/step - loss: 0.1154 - acc: 0.4708 - val_loss: 0.1102 - val_acc: 0.4739\n",
      "Epoch 16/50\n",
      "110288/110288 [==============================] - 148s 1ms/step - loss: 0.1036 - acc: 0.4755 - val_loss: 0.0936 - val_acc: 0.4806\n",
      "Epoch 17/50\n",
      "110288/110288 [==============================] - 146s 1ms/step - loss: 0.0929 - acc: 0.4794 - val_loss: 0.0973 - val_acc: 0.4761\n",
      "Epoch 18/50\n",
      "110288/110288 [==============================] - 147s 1ms/step - loss: 0.0835 - acc: 0.4827 - val_loss: 0.0923 - val_acc: 0.4772\n",
      "Epoch 19/50\n",
      "110288/110288 [==============================] - 146s 1ms/step - loss: 0.0753 - acc: 0.4855 - val_loss: 0.0738 - val_acc: 0.4848\n",
      "Epoch 20/50\n",
      "110288/110288 [==============================] - 146s 1ms/step - loss: 0.0683 - acc: 0.4877 - val_loss: 0.0791 - val_acc: 0.4808\n",
      "Epoch 21/50\n",
      "110288/110288 [==============================] - 150s 1ms/step - loss: 0.0620 - acc: 0.4898 - val_loss: 0.0694 - val_acc: 0.4855\n",
      "Epoch 22/50\n",
      "110288/110288 [==============================] - 148s 1ms/step - loss: 0.0558 - acc: 0.4917 - val_loss: 0.0513 - val_acc: 0.4929\n",
      "Epoch 23/50\n",
      "110288/110288 [==============================] - 149s 1ms/step - loss: 0.0508 - acc: 0.4931 - val_loss: 0.0470 - val_acc: 0.4940\n",
      "Epoch 24/50\n",
      "110288/110288 [==============================] - 148s 1ms/step - loss: 0.0470 - acc: 0.4942 - val_loss: 0.0437 - val_acc: 0.4949\n",
      "Epoch 25/50\n",
      "110288/110288 [==============================] - 147s 1ms/step - loss: 0.0439 - acc: 0.4951 - val_loss: 0.0414 - val_acc: 0.4954\n",
      "Epoch 26/50\n",
      "110288/110288 [==============================] - 146s 1ms/step - loss: 0.0412 - acc: 0.4958 - val_loss: 0.0407 - val_acc: 0.4954\n",
      "Epoch 27/50\n",
      "110288/110288 [==============================] - 149s 1ms/step - loss: 0.0388 - acc: 0.4966 - val_loss: 0.0403 - val_acc: 0.4956\n",
      "Epoch 28/50\n",
      "110288/110288 [==============================] - 147s 1ms/step - loss: 0.0368 - acc: 0.4972 - val_loss: 0.0365 - val_acc: 0.4966\n",
      "Epoch 29/50\n",
      "110288/110288 [==============================] - 149s 1ms/step - loss: 0.0350 - acc: 0.4977 - val_loss: 0.0331 - val_acc: 0.4977\n",
      "Epoch 30/50\n",
      "110288/110288 [==============================] - 145s 1ms/step - loss: 0.0332 - acc: 0.4982 - val_loss: 0.0355 - val_acc: 0.4965\n",
      "Epoch 31/50\n",
      "110288/110288 [==============================] - 146s 1ms/step - loss: 0.0317 - acc: 0.4986 - val_loss: 0.0447 - val_acc: 0.4930\n",
      "Epoch 32/50\n",
      "110288/110288 [==============================] - 146s 1ms/step - loss: 0.0305 - acc: 0.4990 - val_loss: 0.0355 - val_acc: 0.4964\n",
      "Epoch 33/50\n",
      "110288/110288 [==============================] - 146s 1ms/step - loss: 0.0292 - acc: 0.4994 - val_loss: 0.0301 - val_acc: 0.4985\n",
      "Epoch 34/50\n",
      "110288/110288 [==============================] - 150s 1ms/step - loss: 0.0280 - acc: 0.4998 - val_loss: 0.0292 - val_acc: 0.4987\n",
      "Epoch 35/50\n",
      "110288/110288 [==============================] - 148s 1ms/step - loss: 0.0269 - acc: 0.5000 - val_loss: 0.0269 - val_acc: 0.4994\n",
      "Epoch 36/50\n",
      "110288/110288 [==============================] - 149s 1ms/step - loss: 0.0259 - acc: 0.5004 - val_loss: 0.0275 - val_acc: 0.4991\n",
      "Epoch 37/50\n",
      "110288/110288 [==============================] - 147s 1ms/step - loss: 0.0249 - acc: 0.5007 - val_loss: 0.0273 - val_acc: 0.4991\n",
      "Epoch 38/50\n",
      "110288/110288 [==============================] - 148s 1ms/step - loss: 0.0240 - acc: 0.5009 - val_loss: 0.0353 - val_acc: 0.4963\n",
      "Epoch 39/50\n",
      "110288/110288 [==============================] - 147s 1ms/step - loss: 0.0232 - acc: 0.5011 - val_loss: 0.0243 - val_acc: 0.5001\n",
      "Epoch 40/50\n",
      "110288/110288 [==============================] - 147s 1ms/step - loss: 0.0223 - acc: 0.5014 - val_loss: 0.0263 - val_acc: 0.4993\n",
      "Epoch 41/50\n",
      "110288/110288 [==============================] - 148s 1ms/step - loss: 0.0216 - acc: 0.5016 - val_loss: 0.0233 - val_acc: 0.5004\n",
      "Epoch 42/50\n",
      "110288/110288 [==============================] - 145s 1ms/step - loss: 0.0208 - acc: 0.5018 - val_loss: 0.0227 - val_acc: 0.5006\n",
      "Epoch 43/50\n",
      "110288/110288 [==============================] - 145s 1ms/step - loss: 0.0203 - acc: 0.5020 - val_loss: 0.0253 - val_acc: 0.4997\n",
      "Epoch 44/50\n",
      "110288/110288 [==============================] - 147s 1ms/step - loss: 0.0196 - acc: 0.5022 - val_loss: 0.0209 - val_acc: 0.5011\n",
      "Epoch 45/50\n",
      "110288/110288 [==============================] - 147s 1ms/step - loss: 0.0191 - acc: 0.5024 - val_loss: 0.0215 - val_acc: 0.5009\n",
      "Epoch 46/50\n",
      "110288/110288 [==============================] - 146s 1ms/step - loss: 0.0184 - acc: 0.5026 - val_loss: 0.0218 - val_acc: 0.5008\n",
      "Epoch 47/50\n",
      "110288/110288 [==============================] - 148s 1ms/step - loss: 0.0179 - acc: 0.5027 - val_loss: 0.0198 - val_acc: 0.5014\n",
      "Epoch 48/50\n",
      "110288/110288 [==============================] - 148s 1ms/step - loss: 0.0173 - acc: 0.5029 - val_loss: 0.0263 - val_acc: 0.4994\n",
      "Epoch 49/50\n",
      "110288/110288 [==============================] - 144s 1ms/step - loss: 0.0170 - acc: 0.5030 - val_loss: 0.0196 - val_acc: 0.5015\n",
      "Epoch 50/50\n",
      "110288/110288 [==============================] - 146s 1ms/step - loss: 0.0165 - acc: 0.5032 - val_loss: 0.0184 - val_acc: 0.5020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fb4330362b0>"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=128,\n",
    "          epochs=50,\n",
    "          validation_split=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.  Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "colab_type": "code",
    "id": "14yoZMO7I-UZ",
    "outputId": "8242edb6-a8bd-43bf-8de4-6b5c2eda8ee5"
   },
   "outputs": [],
   "source": [
    "# define the encoder model \n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 252
    },
    "colab_type": "code",
    "id": "14yoZMO7I-UZ",
    "outputId": "8242edb6-a8bd-43bf-8de4-6b5c2eda8ee5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, None, 50)          11350     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                [(None, 50), (None, 50),  20200     \n",
      "=================================================================\n",
      "Total params: 31,550\n",
      "Trainable params: 31,550\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Redefine the decoder model with decoder will be getting below inputs from encoder while in prediction\n",
    "decoder_state_input_h = Input(shape=(50,))\n",
    "decoder_state_input_c = Input(shape=(50,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "final_dex2= dex(decoder_inputs)\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(final_dex2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "# sampling model will take encoder states and decoder_input(seed initially) and output the predictions(french word index) We dont care about decoder_states2\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_token_index = dict(\n",
    "    (i, token) for token, i in input_token_index.items())\n",
    "reverse_target_token_index = dict(\n",
    "    (i, token) for token, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for making predictions using encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Gf3pgn9JF7F"
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = target_token_index['START_']\n",
    "# Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "# Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, 0, :])\n",
    "        sampled_token = reverse_target_token_index[sampled_token_index]\n",
    "        decoded_sentence += ' '+sampled_token\n",
    "# Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_token == '_END' or\n",
    "           len(decoded_sentence) > 31):\n",
    "            stop_condition = True\n",
    "# Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "# Update states\n",
    "        states_value = [h, c]\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a prediction - Language translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "83ksDzTQJllu",
    "outputId": "0c3d5f8d-efb3-40b7-ee99-87fd27c5eb8d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" paris est jamais chaud au mois d' octobre , et il est\""
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_seq = encoder_input_data[14077:14078]\n",
    "decoder_sentence = decode_sequence(input_seq)\n",
    "decoder_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uHQW1aZ7IM_O"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "seq2seq.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
